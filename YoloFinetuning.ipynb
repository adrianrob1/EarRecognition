{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "base_path = \"../datasets/ears/images\"\n",
    "filenames = []\n",
    "for infile in os.listdir(base_path):\n",
    "    filenames.append(\"./images/\" + infile)\n",
    "\n",
    "filenames_df = pd.DataFrame(filenames)\n",
    "filenames_shuffled_df = filenames_df.sample(frac=1, random_state=42) \\\n",
    "                                    .reset_index(drop=True)\n",
    "\n",
    "train_df, validate_df, test_df = np.split(filenames_shuffled_df, [int(.6*len(filenames_shuffled_df)), int(.8*len(filenames_shuffled_df))])\n",
    "\n",
    "train_df.to_csv('../datasets/ears/train_ears.txt', header=None, index=None, sep=' ', mode='a')\n",
    "validate_df.to_csv('../datasets/ears/val_ears.txt', header=None, index=None, sep=' ', mode='a')\n",
    "test_df.to_csv('../datasets/ears/test_ears.txt', header=None, index=None, sep=' ', mode='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move the sets in a different directory\n",
    "import shutil\n",
    "import os \n",
    "\n",
    "#Move images\n",
    "os.mkdir(\"../datasets/ears/images/train\") \n",
    "os.mkdir(\"../datasets/ears/images/val\") \n",
    "os.mkdir(\"../datasets/ears/images/test\") \n",
    "\n",
    "with open('../datasets/ears/train_ears.txt') as f:\n",
    "    for line in f:\n",
    "        fileName = os.path.join(\"../datasets/ears/\", line.replace(\"\\n\", \"\")[2:])\n",
    "        destination = '../datasets/ears/images/train'\n",
    "        shutil.move(fileName, destination)\n",
    "        print(\"Moved: \" + fileName + \" to: \" + destination)\n",
    "\n",
    "with open('../datasets/ears/val_ears.txt') as f:\n",
    "    for line in f:\n",
    "        fileName = os.path.join(\"../datasets/ears/\", line.replace(\"\\n\", \"\")[2:])\n",
    "        destination = '../datasets/ears/images/val'\n",
    "        shutil.move(fileName, destination)\n",
    "        print(\"Moved: \" + fileName + \" to: \" + destination)\n",
    "\n",
    "with open('../datasets/ears/test_ears.txt') as f:\n",
    "    for line in f:\n",
    "        fileName = os.path.join(\"../datasets/ears/\", line.replace(\"\\n\", \"\")[2:])\n",
    "        destination = '../datasets/ears/images/test'\n",
    "        shutil.move(fileName, destination)\n",
    "        print(\"Moved: \" + fileName + \" to: \" + destination)\n",
    "        \n",
    "#Move labels\n",
    "os.mkdir(\"../datasets/ears/labels/train\") \n",
    "os.mkdir(\"../datasets/ears/labels/val\") \n",
    "os.mkdir(\"../datasets/ears/labels/test\") \n",
    "\n",
    "with open('../datasets/ears/train_ears.txt') as f:\n",
    "    for line in f:\n",
    "        fileName = os.path.join(\"../datasets/ears/\", line.replace(\"\\n\", \"\").replace(\"jpg\", \"txt\").replace(\"images\",\"labels\")[2:])\n",
    "        destination = '../datasets/ears/labels/train'\n",
    "        shutil.move(fileName, destination)\n",
    "        print(\"Moved: \" + fileName + \" to: \" + destination)\n",
    "\n",
    "with open('../datasets/ears/val_ears.txt') as f:\n",
    "    for line in f:\n",
    "        fileName = os.path.join(\"../datasets/ears/\", line.replace(\"\\n\", \"\").replace(\"jpg\", \"txt\").replace(\"images\",\"labels\")[2:])\n",
    "        destination = '../datasets/ears/labels/val'\n",
    "        shutil.move(fileName, destination)\n",
    "        print(\"Moved: \" + fileName + \" to: \" + destination)\n",
    "\n",
    "with open('../datasets/ears/test_ears.txt') as f:\n",
    "    for line in f:\n",
    "        fileName = os.path.join(\"../datasets/ears/\", line.replace(\"\\n\", \"\").replace(\"jpg\", \"txt\").replace(\"images\",\"labels\")[2:])\n",
    "        destination = '../datasets/ears/labels/test'\n",
    "        shutil.move(fileName, destination)\n",
    "        print(\"Moved: \" + fileName + \" to: \" + destination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the txts\n",
    "search_text = \"/images/\"\n",
    "data = \"\"\n",
    "\n",
    "with open('../datasets/ears/train_ears.txt') as f:\n",
    "    data = f.read()\n",
    "    data = data.replace(search_text, search_text + \"train/\")\n",
    "    \n",
    "with open('../datasets/ears/train_ears.txt', 'w') as f:\n",
    "    # Writing the replaced data in our\n",
    "    # text file\n",
    "    f.write(data)\n",
    "\n",
    "with open('../datasets/ears/val_ears.txt') as f:\n",
    "    data = f.read()\n",
    "    data = data.replace(search_text, search_text + \"val/\")\n",
    "    \n",
    "with open('../datasets/ears/val_ears.txt', 'w') as f:\n",
    "    # Writing the replaced data in our\n",
    "    # text file\n",
    "    f.write(data)\n",
    "    \n",
    "with open('../datasets/ears/test_ears.txt') as f:\n",
    "    data = f.read()\n",
    "    data = data.replace(search_text, search_text + \"test/\")\n",
    "    \n",
    "with open('../datasets/ears/test_ears.txt', 'w') as f:\n",
    "    # Writing the replaced data in our\n",
    "    # text file\n",
    "    f.write(data)\n",
    "                        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7mGmQbAO5pQb"
   },
   "source": [
    "# Setup\n",
    "\n",
    "Clone repo, install dependencies and check PyTorch and GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wbvMlHd_QwMG",
    "outputId": "3809e5a9-dd41-4577-fe62-5531abf7cca2"
   },
   "outputs": [],
   "source": [
    "#!git clone https://github.com/ultralytics/yolov5  # clone\n",
    "#%cd yolov5\n",
    "#%pip install -qr requirements.txt  # install\n",
    "\n",
    "import torch\n",
    "import utils\n",
    "display = utils.notebook_init()  # checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.zeros(1).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from IPython.display import Image  # for displaying images\n",
    "\n",
    "print('torch %s %s' % (torch.__version__, torch.cuda.get_device_properties(0) if torch.cuda.is_available() else 'CPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zR9ZbuQCH7FX",
    "outputId": "8f7e6588-215d-4ebd-93af-88b871e770a7"
   },
   "outputs": [],
   "source": [
    "!python detect.py --weights yolov5s.pt --img 640 --conf 0.25 --source data/images\n",
    "display.Image(filename='runs/detect/exp/zidane.jpg', width=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2fLAV42oNb7M"
   },
   "outputs": [],
   "source": [
    "# Weights & Biases  (optional)\n",
    "import wandb\n",
    "wandb.login(key=\"KEY_WANDB_SECRET\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nano training\n",
    "!python train.py --img 640 --batch-size 16 --epochs 30 --data ears.yaml --weights yolov5n.pt --device 0\n",
    "\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run inference on a test model\n",
    "!python detect.py --weights runs/train/exp18/weights/best.pt --img 640 --conf 0.1 --source ../datasets/ears/images/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#display inference on ALL test images\n",
    "\n",
    "import glob\n",
    "from IPython.display import Image, display\n",
    "\n",
    "i = 0\n",
    "for imageName in glob.glob('runs/detect/exp7/*.jpg'): #assuming JPG\n",
    "    display(Image(filename=imageName))\n",
    "    print(\"\\n\")\n",
    "    i+=1\n",
    "    if i == 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mexport: \u001b[0mdata=data/ears.yaml, weights=['runs/train/exp33/weights/best.pt'], imgsz=[640], batch_size=1, device=0, half=False, inplace=False, train=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=12, verbose=False, workspace=4, nms=False, agnostic_nms=False, topk_per_class=100, topk_all=100, iou_thres=0.45, conf_thres=0.25, include=['onnx']\n",
      "YOLOv5  2022-1-28 torch 1.10.2+cu113 CUDA:0 (NVIDIA GeForce GTX 1070, 8192MiB)\n",
      "\n",
      "Fusing layers... \n",
      "Model Summary: 213 layers, 1761871 parameters, 0 gradients, 4.2 GFLOPs\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from runs\\train\\exp33\\weights\\best.pt (3.8 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.10.2...\n",
      "F:\\face_recognition\\yolov5\\models\\yolo.py:57: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if self.onnx_dynamic or self.grid[i].shape[2:4] != x[i].shape[2:4]:\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m export success, saved as runs\\train\\exp33\\weights\\best.onnx (7.5 MB)\n",
      "\n",
      "Export complete (6.67s)\n",
      "Results saved to \u001b[1mF:\\face_recognition\\yolov5\\runs\\train\\exp33\\weights\u001b[0m\n",
      "Visualize with https://netron.app\n",
      "Detect with `python detect.py --weights runs\\train\\exp33\\weights\\best.onnx` or `model = torch.hub.load('ultralytics/yolov5', 'custom', 'runs\\train\\exp33\\weights\\best.onnx')\n",
      "Validate with `python val.py --weights runs\\train\\exp33\\weights\\best.onnx`\n"
     ]
    }
   ],
   "source": [
    "#Export to ONNX format\n",
    "!python export.py --data data/ears.yaml --weights runs/train/exp33/weights/best.pt --img 640 --batch 1 --device 0 --include onnx "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(1, 640, 640, 3)]   0           []                               \n",
      "                                                                                                  \n",
      " tf_conv (TFConv)               (1, 320, 320, 16)    1744        ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " tf_conv_1 (TFConv)             (1, 160, 160, 32)    4640        ['tf_conv[0][0]']                \n",
      "                                                                                                  \n",
      " tfc3 (TFC3)                    (1, 160, 160, 32)    4704        ['tf_conv_1[0][0]']              \n",
      "                                                                                                  \n",
      " tf_conv_7 (TFConv)             (1, 80, 80, 64)      18496       ['tfc3[0][0]']                   \n",
      "                                                                                                  \n",
      " tfc3_1 (TFC3)                  (1, 80, 80, 64)      28928       ['tf_conv_7[0][0]']              \n",
      "                                                                                                  \n",
      " tf_conv_15 (TFConv)            (1, 40, 40, 128)     73856       ['tfc3_1[0][0]']                 \n",
      "                                                                                                  \n",
      " tfc3_2 (TFC3)                  (1, 40, 40, 128)     156288      ['tf_conv_15[0][0]']             \n",
      "                                                                                                  \n",
      " tf_conv_25 (TFConv)            (1, 20, 20, 256)     295168      ['tfc3_2[0][0]']                 \n",
      "                                                                                                  \n",
      " tfc3_3 (TFC3)                  (1, 20, 20, 256)     295680      ['tf_conv_25[0][0]']             \n",
      "                                                                                                  \n",
      " tfsppf (TFSPPF)                (1, 20, 20, 256)     164224      ['tfc3_3[0][0]']                 \n",
      "                                                                                                  \n",
      " tf_conv_33 (TFConv)            (1, 20, 20, 128)     32896       ['tfsppf[0][0]']                 \n",
      "                                                                                                  \n",
      " tf_upsample (TFUpsample)       (1, 40, 40, 128)     0           ['tf_conv_33[0][0]']             \n",
      "                                                                                                  \n",
      " tf_concat (TFConcat)           (1, 40, 40, 256)     0           ['tf_upsample[0][0]',            \n",
      "                                                                  'tfc3_2[0][0]']                 \n",
      "                                                                                                  \n",
      " tfc3_4 (TFC3)                  (1, 40, 40, 128)     90496       ['tf_concat[0][0]']              \n",
      "                                                                                                  \n",
      " tf_conv_39 (TFConv)            (1, 40, 40, 64)      8256        ['tfc3_4[0][0]']                 \n",
      "                                                                                                  \n",
      " tf_upsample_1 (TFUpsample)     (1, 80, 80, 64)      0           ['tf_conv_39[0][0]']             \n",
      "                                                                                                  \n",
      " tf_concat_1 (TFConcat)         (1, 80, 80, 128)     0           ['tf_upsample_1[0][0]',          \n",
      "                                                                  'tfc3_1[0][0]']                 \n",
      "                                                                                                  \n",
      " tfc3_5 (TFC3)                  (1, 80, 80, 64)      22720       ['tf_concat_1[0][0]']            \n",
      "                                                                                                  \n",
      " tf_conv_45 (TFConv)            (1, 40, 40, 64)      36928       ['tfc3_5[0][0]']                 \n",
      "                                                                                                  \n",
      " tf_concat_2 (TFConcat)         (1, 40, 40, 128)     0           ['tf_conv_45[0][0]',             \n",
      "                                                                  'tf_conv_39[0][0]']             \n",
      "                                                                                                  \n",
      " tfc3_6 (TFC3)                  (1, 40, 40, 128)     74112       ['tf_concat_2[0][0]']            \n",
      "                                                                                                  \n",
      " tf_conv_51 (TFConv)            (1, 20, 20, 128)     147584      ['tfc3_6[0][0]']                 \n",
      "                                                                                                  \n",
      " tf_concat_3 (TFConcat)         (1, 20, 20, 256)     0           ['tf_conv_51[0][0]',             \n",
      "                                                                  'tf_conv_33[0][0]']             \n",
      "                                                                                                  \n",
      " tfc3_7 (TFC3)                  (1, 20, 20, 256)     295680      ['tf_concat_3[0][0]']            \n",
      "                                                                                                  \n",
      " tf_detect (TFDetect)           ((1, 25200, 7),      9471        ['tfc3_5[0][0]',                 \n",
      "                                 [(1, 3, 6400, 7),                'tfc3_6[0][0]',                 \n",
      "                                 (1, 3, 1600, 7),                 'tfc3_7[0][0]']                 \n",
      "                                 (1, 3, 400, 7)])                                                 \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,761,871\n",
      "Trainable params: 0\n",
      "Non-trainable params: 1,761,871\n",
      "__________________________________________________________________________________________________\n",
      "Estimated count of arithmetic ops: 4.930 G  ops, equivalently 2.465 G  MACs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mexport: \u001b[0mdata=data/ears.yaml, weights=['runs/train/exp33/weights/best.pt'], imgsz=[640], batch_size=1, device=cpu, half=False, inplace=False, train=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=12, verbose=False, workspace=4, nms=False, agnostic_nms=False, topk_per_class=100, topk_all=100, iou_thres=0.45, conf_thres=0.25, include=['tflite']\n",
      "YOLOv5  2022-1-28 torch 1.10.2+cu113 CPU\n",
      "\n",
      "Fusing layers... \n",
      "Model Summary: 213 layers, 1761871 parameters, 0 gradients, 4.2 GFLOPs\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from runs\\train\\exp33\\weights\\best.pt (3.8 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mTensorFlow SavedModel:\u001b[0m starting export with tensorflow 2.7.0...\n",
      "\n",
      "                 from  n    params  module                                  arguments                     \n",
      "2022-02-02 20:37:24.705256: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2022-02-02 20:37:24.712279: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: LAPTOP-N16NAPP9\n",
      "2022-02-02 20:37:24.712512: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: LAPTOP-N16NAPP9\n",
      "2022-02-02 20:37:24.754217: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX AVX2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "  0                -1  1      1760  models.common.Conv                      [3, 16, 6, 2, 2]              \n",
      "  1                -1  1      4672  models.common.Conv                      [16, 32, 3, 2]                \n",
      "  2                -1  1      4800  models.common.C3                        [32, 32, 1]                   \n",
      "  3                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
      "  4                -1  1     29184  models.common.C3                        [64, 64, 2]                   \n",
      "  5                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
      "  6                -1  1    156928  models.common.C3                        [128, 128, 3]                 \n",
      "  7                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
      "  8                -1  1    296448  models.common.C3                        [256, 256, 1]                 \n",
      "  9                -1  1    164608  models.common.SPPF                      [256, 256, 5]                 \n",
      " 10                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
      " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
      " 13                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n",
      " 14                -1  1      8320  models.common.Conv                      [128, 64, 1, 1]               \n",
      " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
      " 17                -1  1     22912  models.common.C3                        [128, 64, 1, False]           \n",
      " 18                -1  1     36992  models.common.Conv                      [64, 64, 3, 2]                \n",
      " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
      " 20                -1  1     74496  models.common.C3                        [128, 128, 1, False]          \n",
      " 21                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
      " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
      " 23                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n",
      " 24      [17, 20, 23]  1      9471  models.yolo.Detect                      [2, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [64, 128, 256], [640, 640]]\n",
      "Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "2022-02-02 20:37:35.017636: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "Found untraced functions such as tf_conv_2_layer_call_fn, tf_conv_2_layer_call_and_return_conditional_losses, tf_conv_3_layer_call_fn, tf_conv_3_layer_call_and_return_conditional_losses, tf_conv_4_layer_call_fn while saving (showing 5 of 520). These functions will not be directly callable after loading.\n",
      "Assets written to: runs\\train\\exp33\\weights\\best_saved_model\\assets\n",
      "\u001b[34m\u001b[1mTensorFlow SavedModel:\u001b[0m export success, saved as runs\\train\\exp33\\weights\\best_saved_model (62.1 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mTensorFlow Lite:\u001b[0m starting export with tensorflow 2.7.0...\n",
      "Found untraced functions such as tf_conv_2_layer_call_fn, tf_conv_2_layer_call_and_return_conditional_losses, tf_conv_3_layer_call_fn, tf_conv_3_layer_call_and_return_conditional_losses, tf_conv_4_layer_call_fn while saving (showing 5 of 520). These functions will not be directly callable after loading.\n",
      "Assets written to: C:\\Users\\anton\\AppData\\Local\\Temp\\tmpc8z7mamh\\assets\n",
      "2022-02-02 20:38:38.559835: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:363] Ignored output_format.\n",
      "2022-02-02 20:38:38.559877: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:366] Ignored drop_control_dependency.\n",
      "2022-02-02 20:38:38.571757: I tensorflow/cc/saved_model/reader.cc:43] Reading SavedModel from: C:\\Users\\anton\\AppData\\Local\\Temp\\tmpc8z7mamh\n",
      "2022-02-02 20:38:38.648008: I tensorflow/cc/saved_model/reader.cc:107] Reading meta graph with tags { serve }\n",
      "2022-02-02 20:38:38.648069: I tensorflow/cc/saved_model/reader.cc:148] Reading SavedModel debug info (if present) from: C:\\Users\\anton\\AppData\\Local\\Temp\\tmpc8z7mamh\n",
      "2022-02-02 20:38:39.040425: I tensorflow/cc/saved_model/loader.cc:210] Restoring SavedModel bundle.\n",
      "2022-02-02 20:38:39.964820: I tensorflow/cc/saved_model/loader.cc:194] Running initialization op on SavedModel bundle at path: C:\\Users\\anton\\AppData\\Local\\Temp\\tmpc8z7mamh\n",
      "2022-02-02 20:38:40.405513: I tensorflow/cc/saved_model/loader.cc:283] SavedModel load for tags { serve }; Status: success: OK. Took 1834175 microseconds.\n",
      "2022-02-02 20:38:41.436521: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-02-02 20:38:42.556049: I tensorflow/compiler/mlir/lite/flatbuffer_export.cc:1962] Estimated count of arithmetic ops: 4.930 G  ops, equivalently 2.465 G  MACs\n",
      "\n",
      "\u001b[34m\u001b[1mTensorFlow Lite:\u001b[0m export success, saved as runs\\train\\exp33\\weights\\best-fp16.tflite (3.7 MB)\n",
      "\n",
      "Export complete (81.97s)\n",
      "Results saved to \u001b[1mF:\\face_recognition\\yolov5\\runs\\train\\exp33\\weights\u001b[0m\n",
      "Visualize with https://netron.app\n",
      "Detect with `python detect.py --weights runs\\train\\exp33\\weights\\best-fp16.tflite` or `model = torch.hub.load('ultralytics/yolov5', 'custom', 'runs\\train\\exp33\\weights\\best-fp16.tflite')\n",
      "Validate with `python val.py --weights runs\\train\\exp33\\weights\\best-fp16.tflite`\n"
     ]
    }
   ],
   "source": [
    "#Export to Tflite format\n",
    "!python export.py --data data/ears.yaml --weights runs/train/exp33/weights/best.pt --img 640 --batch 1 --device cpu --include tflite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mdetect: \u001b[0mweights=['runs/train/exp33/weights/best.pt'], source=../datasets/ears/images/test_various, data=data/ears.yaml, imgsz=[640, 640], conf_thres=0.1, iou_thres=0.45, max_det=1000, device=0, view_img=False, save_txt=False, save_conf=False, save_crop=False, nosave=False, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=runs\\detect, name=exp, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False\n",
      "YOLOv5  2022-1-28 torch 1.10.2+cu113 CUDA:0 (NVIDIA GeForce GTX 1070, 8192MiB)\n",
      "\n",
      "Fusing layers... \n",
      "Model Summary: 213 layers, 1761871 parameters, 0 gradients, 4.2 GFLOPs\n",
      "image 1/14 F:\\face_recognition\\datasets\\ears\\images\\test_various\\002_left_e1ar.jpg: 640x608 1 earRight, Done. (0.031s)\n",
      "image 2/14 F:\\face_recognition\\datasets\\ears\\images\\test_various\\002_left_ear.jpg: 640x480 Done. (0.031s)\n",
      "image 3/14 F:\\face_recognition\\datasets\\ears\\images\\test_various\\005222_up_ear.jpg: 640x480 1 earRight, Done. (0.016s)\n",
      "image 4/14 F:\\face_recognition\\datasets\\ears\\images\\test_various\\005_up_ear.jpg: 640x480 Done. (0.016s)\n",
      "image 5/14 F:\\face_recognition\\datasets\\ears\\images\\test_various\\007_back_ear.jpg: 640x480 Done. (0.016s)\n",
      "image 6/14 F:\\face_recognition\\datasets\\ears\\images\\test_various\\007_sdsdback_ear.jpg: 640x480 1 earLeft, Done. (0.016s)\n",
      "image 7/14 F:\\face_recognition\\datasets\\ears\\images\\test_various\\05_head1_shadow_1280_1024.jpg: 640x640 1 earLeft, Done. (0.016s)\n",
      "image 8/14 F:\\face_recognition\\datasets\\ears\\images\\test_various\\05_head1_shadow_1280_eee1024.jpg: 640x640 1 earLeft, Done. (0.016s)\n",
      "image 9/14 F:\\face_recognition\\datasets\\ears\\images\\test_various\\face-model-portrait-photography-actor-profile-hair-nose-Person-Colton-Haynes-head-man-male-hairstyle-portrait-photography-photo-shoot-close-up-facial-hair-561682.jpg: 544x640 1 earRight, Done. (0.016s)\n",
      "image 10/14 F:\\face_recognition\\datasets\\ears\\images\\test_various\\profilo-di-donna-90003cc9-feee-450a-9ffa-aa986ee32fb8.jpg: 640x576 1 earLeft, Done. (0.016s)\n",
      "image 11/14 F:\\face_recognition\\datasets\\ears\\images\\test_various\\profilo-di-donna-90003cc9-feee-450a-9ffa-aa986ee32fb8bw.jpg: 640x576 1 earLeft, Done. (0.016s)\n",
      "image 12/14 F:\\face_recognition\\datasets\\ears\\images\\test_various\\profilo-di-ragazzo-ce520f37-cc66-403d-b380-b8973b6aeee1.jpg: 480x640 1 earLeft, Done. (0.010s)\n",
      "image 13/14 F:\\face_recognition\\datasets\\ears\\images\\test_various\\test-viso-profilo.jpg: 640x640 1 earLeft, Done. (0.016s)\n",
      "image 14/14 F:\\face_recognition\\datasets\\ears\\images\\test_various\\test-viso-profilobw.jpg: 640x640 1 earLeft, Done. (0.016s)\n",
      "Speed: 0.1ms pre-process, 17.4ms inference, 1.1ms NMS per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\exp29\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Run inference on a test model\n",
    "!python detect.py --data data/ears.yaml --weights runs/train/exp33/weights/best.pt --img 640 --conf 0.1 --source ../datasets/ears/images/test_various --device 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create augmented dataset with random crops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "def format_yolov5(frame):\n",
    "    col, row, _ = frame.shape\n",
    "    _max = max(col, row)\n",
    "    result = np.zeros((_max, _max, 3), np.uint8)\n",
    "    result[0:col, 0:row] = frame\n",
    "    return result\n",
    "\n",
    "def predict_yolov5(net, input_image, orig_image):\n",
    "    import random \n",
    "    blob = cv2.dnn.blobFromImage(input_image , 1/255.0, (640, 640), swapRB=True)\n",
    "    net.setInput(blob)\n",
    "    predictions = net.forward()\n",
    "\n",
    "    class_ids = []\n",
    "    confidences = []\n",
    "    boxes = []\n",
    "    crop_boxes = []\n",
    "    crop_areas = []\n",
    "\n",
    "    output_data = predictions[0]\n",
    "    \n",
    "    image_height = input_image.shape[0]\n",
    "    image_width  = input_image.shape[1]\n",
    "    \n",
    "    y_factor =  float(image_height) / float(640.0)\n",
    "    x_factor = float(image_width) / float(640.0)\n",
    "    \n",
    "    for r in range(25200):\n",
    "        row = output_data[r]\n",
    "        confidence = row[4]\n",
    "        if confidence >= 0.4:\n",
    "\n",
    "            classes_scores = row[5:]\n",
    "            _, _, _, max_indx = cv2.minMaxLoc(classes_scores)\n",
    "            class_id = max_indx[1]\n",
    "            if (classes_scores[class_id] > .25):\n",
    "\n",
    "                confidences.append(confidence)\n",
    "\n",
    "                class_ids.append(class_id)\n",
    "\n",
    "                x, y, w, h = row[0].item() , row[1].item() , row[2].item(), row[3].item() \n",
    "                \n",
    "                left   = int((x - 0.5 * w) * x_factor)\n",
    "                top    = int((y - 0.5 * h) * y_factor)\n",
    "                width  = int(w  * x_factor)\n",
    "                height = int(h * y_factor)\n",
    "                \n",
    "                box = np.array([left, top, width, height])\n",
    "                boxes.append(box)\n",
    "\n",
    "                ## CROP THE IMAGE\n",
    "                random.seed()\n",
    "                randoms        = np.array([str(random.random()/2.5) for i in range(4) ])\n",
    "                randoms = np.array([float(ran) for ran in randoms])\n",
    "                neg_randoms    = [1-ran for ran in randoms]\n",
    "\n",
    "                padding_left   = int( left * randoms[0] )\n",
    "                padding_right  = int((image_width - (left + width)) * randoms[1])\n",
    "                padding_top    = int( top * randoms[2] )\n",
    "                padding_bottom = int((image_height - (top + height)) * randoms[3])\n",
    "                \n",
    "                crop_area = np.array([int(top-padding_top), int(top+height+padding_bottom), int(left-padding_left), int(left+width+padding_right)])\n",
    "                crop_areas.append(crop_area)\n",
    "\n",
    "                crop_box = np.array([left-int(neg_randoms[0]*left), top-int((neg_randoms[2])*top), width, height])\n",
    "                crop_boxes.append(crop_box)\n",
    "\n",
    "\n",
    "    indexes = cv2.dnn.NMSBoxes(boxes, confidences, 0.25, 0.45) \n",
    "\n",
    "    result_class_ids   = []\n",
    "    result_confidences = []\n",
    "    result_boxes       = []\n",
    "    result_areas       = []\n",
    "\n",
    "    for i in indexes:\n",
    "        result_confidences.append(confidences[i])\n",
    "        result_class_ids.append(class_ids[i])\n",
    "        result_boxes.append(crop_boxes[i])\n",
    "        result_areas.append(crop_areas[i])\n",
    "        \n",
    "    crop_final = []\n",
    "\n",
    "    max_conf = np.argmax(result_confidences)\n",
    "\n",
    "    box        = result_boxes[max_conf]\n",
    "    class_id   = result_class_ids[max_conf]\n",
    "    crop_final = result_areas[max_conf]\n",
    "\n",
    "    image_cropped = orig_image[crop_final[0]:crop_final[1], crop_final[2]:crop_final[3]]\n",
    "\n",
    "        \n",
    "    #cv2.rectangle(image_cropped, box, (255, 255, 0), 2)\n",
    "\n",
    "    cropped_height, cropped_width = image_cropped.shape[:2]\n",
    "    \n",
    "    if cropped_height < 640 and cropped_width < 640:\n",
    "        import imutils\n",
    "        try:\n",
    "            image_cropped = imutils.resize(image_cropped, width=640)\n",
    "        except:\n",
    "            print(\"Skipping cropping\")\n",
    "    #img2 = image_cropped[:,:,::-1]\n",
    "    #plt.imshow(img2)\n",
    "    #plt.show()\n",
    "    \n",
    "    try: \n",
    "        label_width  = box[2] / cropped_width\n",
    "        label_heigth = box[3] / cropped_height\n",
    "\n",
    "        label_left   = box[0] / cropped_width  + 0.5 * label_width\n",
    "        label_top    = box[1] / cropped_height + 0.5 * label_heigth\n",
    "\n",
    "        txt_content = \"{} {} {} {} {}\".format(class_id, label_left, label_top, label_width, label_heigth)\n",
    "    except: \n",
    "        print(\"Null values\")\n",
    "        return None, \"\"\n",
    "    return image_cropped, txt_content\n",
    "    \n",
    "    \n",
    "def elements(array):\n",
    "    return array.ndim and array.size\n",
    "    \n",
    "def crop_dataset(net):\n",
    "    import os\n",
    "    i = 0\n",
    "    checkpoint = 0\n",
    "    print(\"Training set\")\n",
    "    with open('../datasets/ears/train_ears.txt') as f:\n",
    "        for line in f:\n",
    "            fileName = os.path.join(\"../datasets/ears/\", line.replace(\"\\n\", \"\"))      \n",
    "            image = cv2.imread(fileName)\n",
    "            input_image = format_yolov5(image) \n",
    "            image_cropped, txt_content = predict_yolov5(net, input_image, image)\n",
    "            if image_cropped is not None and elements(image_cropped) > 0:\n",
    "                cv2.imwrite('../datasets/ears_cropped/images/train/'+ str(i)+\".jpg\",image_cropped)\n",
    "                with open('../datasets/ears_cropped/labels/train/' + str(i) + '.txt', 'w') as output:\n",
    "                    output.write(txt_content)\n",
    "                with open('../datasets/ears_cropped/train_ear.txt', 'w') as fileList:\n",
    "                    fileList.writelines(\"./images/train/%s.jpg\\n\" % j for j in range(checkpoint, i))\n",
    "            i+=1\n",
    "            \n",
    "    checkpoint = i\n",
    "    print(\"Validation set\")\n",
    "    with open('../datasets/ears/val_ears.txt') as f:\n",
    "        for line in f:\n",
    "            fileName = os.path.join(\"../datasets/ears/\", line.replace(\"\\n\", \"\"))      \n",
    "            image = cv2.imread(fileName)\n",
    "            input_image = format_yolov5(image) \n",
    "            image_cropped, txt_content = predict_yolov5(net, input_image, image)\n",
    "            if image_cropped is not None and elements(image_cropped) > 0:\n",
    "                cv2.imwrite('../datasets/ears_cropped/images/val/'+ str(i)+\".jpg\",image_cropped)\n",
    "                with open('../datasets/ears_cropped/labels/val/' + str(i) + '.txt', 'w') as output:\n",
    "                    output.write(txt_content)\n",
    "                with open('../datasets/ears_cropped/val_ear.txt', 'w') as fileList:\n",
    "                    fileList.writelines(\"./images/val/%s.jpg\\n\" % j for j in range(checkpoint, i))\n",
    "            i+=1\n",
    "    checkpoint = i    \n",
    "    \n",
    "    print(\"Test set\")\n",
    "    with open('../datasets/ears/test_ears.txt') as f:\n",
    "        for line in f:\n",
    "            fileName = os.path.join(\"../datasets/ears/\", line.replace(\"\\n\", \"\"))      \n",
    "            image = cv2.imread(fileName)\n",
    "            input_image = format_yolov5(image) \n",
    "            image_cropped, txt_content = predict_yolov5(net, input_image, image)\n",
    "            if image_cropped is not None and elements(image_cropped) > 0:\n",
    "                cv2.imwrite('../datasets/ears_cropped/images/test/'+ str(i)+\".jpg\",image_cropped)\n",
    "                with open('../datasets/ears_cropped/labels/test/' + str(i) + '.txt', 'w') as output:\n",
    "                    output.write(txt_content)\n",
    "                with open('../datasets/ears_cropped/test_ear.txt', 'w') as fileList:\n",
    "                    fileList.writelines(\"./images/test/%s.jpg\\n\" % j for j in range(checkpoint, i))\n",
    "            i+=1\n",
    "            \n",
    "    print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model on opencv\n",
    "import cv2\n",
    "import os\n",
    "net = cv2.dnn.readNet('runs/train/exp31/weights/best.onnx')\n",
    "\n",
    "os.mkdir(\"../datasets/ears_cropped/\")\n",
    "os.mkdir(\"../datasets/ears_cropped/images\")\n",
    "os.mkdir(\"../datasets/ears_cropped/labels\") \n",
    "\n",
    "os.mkdir(\"../datasets/ears_cropped/images/train\") \n",
    "os.mkdir(\"../datasets/ears_cropped/images/val\") \n",
    "os.mkdir(\"../datasets/ears_cropped/images/test\") \n",
    "\n",
    "os.mkdir(\"../datasets/ears_cropped/labels/train\") \n",
    "os.mkdir(\"../datasets/ears_cropped/labels/val\") \n",
    "os.mkdir(\"../datasets/ears_cropped/labels/test\") \n",
    "\n",
    "crop_dataset(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetune model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep training the model with image cropped and lower learning rate\n",
    "!python train.py --img 640 --batch-size 32 --epochs 30 --data ears_cropped.yaml --weights runs/train/exp31/weights/best.pt --device 0"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "YOLOv5 Tutorial",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0ce7164fc0c74bb9a2b5c7037375a727": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "25621cff5d16448cb7260e839fd0f543": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "384a001876054c93b0af45cd1e960bfe": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_25621cff5d16448cb7260e839fd0f543",
      "placeholder": "​",
      "style": "IPY_MODEL_9f09facb2a6c4a7096810d327c8b551c",
      "value": "100%"
     }
    },
    "473371611126476c88d5d42ec7031ed6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5296d28be75740b2892ae421bbec3657": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_65efdfd0d26c46e79c8c5ff3b77126cc",
      "placeholder": "​",
      "style": "IPY_MODEL_473371611126476c88d5d42ec7031ed6",
      "value": " 780M/780M [00:11&lt;00:00, 91.9MB/s]"
     }
    },
    "65efdfd0d26c46e79c8c5ff3b77126cc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "769ecde6f2e64bacb596ce972f8d3d2d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9f09facb2a6c4a7096810d327c8b551c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c4c4593c10904cb5b8a5724d60c7e181": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dded0aeae74440f7ba2ffa0beb8dd612": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c4c4593c10904cb5b8a5724d60c7e181",
      "max": 818322941,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_0ce7164fc0c74bb9a2b5c7037375a727",
      "value": 818322941
     }
    },
    "eb95db7cae194218b3fcefb439b6352f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_384a001876054c93b0af45cd1e960bfe",
       "IPY_MODEL_dded0aeae74440f7ba2ffa0beb8dd612",
       "IPY_MODEL_5296d28be75740b2892ae421bbec3657"
      ],
      "layout": "IPY_MODEL_769ecde6f2e64bacb596ce972f8d3d2d"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
